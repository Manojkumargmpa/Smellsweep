[33mcommit 20b2e65ce1ed5ff956ce1f8410c6842babbf0e80[m
Author: ronin <vikrama.nallapu@gmail.com>
Date:   Mon Mar 11 18:54:33 2024 +0530

    mycodes

[1mdiff --git a/backend/app.py b/backend/app.py[m
[1mindex 7d64f25..af3456b 100644[m
[1m--- a/backend/app.py[m
[1m+++ b/backend/app.py[m
[36m@@ -1,8 +1,17 @@[m
 from flask import Flask, request, jsonify[m
 import pandas as pd[m
 from flask_cors import CORS[m
[32m+[m[32mfrom fuzzywuzzy import fuzz[m
 import os[m
 from datasmells_algorithms.Tejeswar_smells.dummy_value import identify_dummy_values[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.duplicate_value import detect_and_report_duplicate_data[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.missing_value import detect_and_report_missing_data[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.extreme_value import extreme_values[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.mis_spelling import detect_misspelling_data_smell[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.suspectclass_value import detect_and_report_suspect_class_values[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.casing_value import detect_and_report_casing_data_smells[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.longdata_value import detect_and_report_long_data_values[m
[32m+[m[32mfrom datasmells_algorithms.Vikram_smells.ambiguous_value import detect_ambiguous_values[m
 [m
 app = Flask(__name__)[m
 CORS(app)[m
[36m@@ -12,6 +21,14 @@[m [mdef process_dataframe(df):[m
         # Call the identify_dummy_values function[m
         aggregated_metrics = {[m
             'dummy_values':  identify_dummy_values(df),[m
[32m+[m[32m            'duplicate_values': detect_and_report_duplicate_data(df),[m
[32m+[m[32m            'missing_value': detect_and_report_missing_data(df),[m
[32m+[m[32m            'extreme_value': extreme_values(df),[m
[32m+[m[32m            'mis_spelling': detect_misspelling_data_smell(df),[m
[32m+[m[32m            'suspect_class': detect_and_report_suspect_class_values(df),[m
[32m+[m[32m            'casing_value': detect_and_report_casing_data_smells(df),[m
[32m+[m[32m            'longdata_value': detect_and_report_long_data_values(df),[m
[32m+[m[32m            'ambiguous_value': detect_ambiguous_values(df),[m
             # Add metrics from other algorithms here[m
         }[m
 [m
[1mdiff --git a/backend/datasmells_algorithms/Tejeswar_smells/__pycache__/dummy_value.cpython-312.pyc b/backend/datasmells_algorithms/Tejeswar_smells/__pycache__/dummy_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..1c92582[m
Binary files /dev/null and b/backend/datasmells_algorithms/Tejeswar_smells/__pycache__/dummy_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/ambiguous_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/ambiguous_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..fa04a0e[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/ambiguous_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/casing_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/casing_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..514ed00[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/casing_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/duplicate_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/duplicate_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..03b2bb6[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/duplicate_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/extreme_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/extreme_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..0350d9c[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/extreme_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/longdata_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/longdata_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..e4f7a9e[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/longdata_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/mis_spelling.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/mis_spelling.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..1424952[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/mis_spelling.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/missing_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/missing_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..0b3f459[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/missing_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/__pycache__/suspectclass_value.cpython-312.pyc b/backend/datasmells_algorithms/Vikram_smells/__pycache__/suspectclass_value.cpython-312.pyc[m
[1mnew file mode 100644[m
[1mindex 0000000..87c6889[m
Binary files /dev/null and b/backend/datasmells_algorithms/Vikram_smells/__pycache__/suspectclass_value.cpython-312.pyc differ
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/ambiguous_value.py b/backend/datasmells_algorithms/Vikram_smells/ambiguous_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..46938eb[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/ambiguous_value.py[m
[36m@@ -0,0 +1,56 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m[32mimport numpy as np[m
[32m+[m
[32m+[m[32m# Function to detect ambiguous values in text columns[m
[32m+[m[32mdef detect_ambiguous_values(df):[m
[32m+[m[32m    ambiguous_rows = [][m
[32m+[m[32m    for index, row in df.iterrows():[m
[32m+[m[32m        for col in df.columns:[m
[32m+[m[32m            if pd.api.types.is_string_dtype(df[col]):[m
[32m+[m[32m                if is_ambiguous(row[col]):[m
[32m+[m[32m                    ambiguous_rows.append(index)[m
[32m+[m[32m                    break[m
[32m+[m[32m    return ambiguous_rows[m
[32m+[m
[32m+[m[32m# Rule-based detection: Check if the text contains ambiguous keywords[m
[32m+[m[32mdef is_ambiguous(text):[m
[32m+[m[32m    ambiguous_keywords = ['unclear', 'vague', 'ambiguous', 'confusing', 'uninterpretable'][m
[32m+[m[32m    for keyword in ambiguous_keywords:[m
[32m+[m[32m        if keyword in text.lower():[m
[32m+[m[32m            return True[m
[32m+[m[32m    return False[m
[32m+[m
[32m+[m[32m# Function to calculate metrics for ambiguous values[m
[32m+[m[32mdef ambiguous_value_metrics(df, ambiguous_rows):[m
[32m+[m[32m    num_ambiguous = len(ambiguous_rows)[m
[32m+[m[32m    ambiguous_percentage = (num_ambiguous / len(df)) * 100[m
[32m+[m[32m    ambiguous_columns = df.columns[df.isin(ambiguous_rows).any()][m
[32m+[m[32m    num_ambiguous_columns = len(ambiguous_columns)[m
[32m+[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'num_ambiguous_rows': num_ambiguous,[m
[32m+[m[32m        'ambiguous_percentage': ambiguous_percentage,[m
[32m+[m[32m        'ambiguous_columns': ambiguous_columns.tolist(),[m
[32m+[m[32m        'num_ambiguous_columns': num_ambiguous_columns[m
[32m+[m[32m    }[m
[32m+[m[32m    return metrics[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# Assuming 'df' is your DataFrame[m
[32m+[m[32m# df = pd.DataFrame({'Text': ['This is clear', 'This is vague', 'Confusing text', 'Not sure']})[m
[32m+[m[32m# ambiguous_rows = detect_ambiguous_values(df)[m
[32m+[m[32m# metrics = ambiguous_value_metrics(df, ambiguous_rows)[m
[32m+[m[32m# print(metrics)[m
[32m+[m
[32m+[m[32m# Now, integrate these functions into the existing extreme_values function[m
[32m+[m[32mdef ambiguous_values(df):[m
[32m+[m[32m    ambiguous_rows = detect_ambiguous_values(df)[m
[32m+[m[32m    if not ambiguous_rows:[m
[32m+[m[32m        return "No ambiguous values detected in the dataset."[m
[32m+[m[32m    else:[m
[32m+[m[32m        metrics = ambiguous_value_metrics(df, ambiguous_rows)[m
[32m+[m[32m        return metrics[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# instr = ambiguous_values(df)[m
[32m+[m[32m# print(instr)[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/casing_value.py b/backend/datasmells_algorithms/Vikram_smells/casing_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..ab089d8[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/casing_value.py[m
[36m@@ -0,0 +1,28 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m
[32m+[m[32mdef detect_and_report_casing_data_smells(data):[m
[32m+[m[32m    # Initialize metrics dictionary to store results[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'Casing inconsistency count per attribute': {},[m
[32m+[m[32m        'Casing inconsistency percentage per attribute': {}[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    # Loop through columns to check for casing inconsistency[m
[32m+[m[32m    for column in data.columns:[m
[32m+[m[32m        if data[column].dtype == 'object':  # Check if the column contains string data[m
[32m+[m[32m            unique_values = data[column].unique()[m
[32m+[m[32m            casing_inconsistency_count = sum(1 for value in unique_values if isinstance(value, str) and (value.lower() != value and value.upper() != value))[m
[32m+[m[32m            metrics['Casing inconsistency count per attribute'][column] = casing_inconsistency_count[m
[32m+[m[32m            total_unique_values = len(unique_values)[m
[32m+[m[32m            if total_unique_values > 0:[m
[32m+[m[32m                casing_inconsistency_percentage = (casing_inconsistency_count / total_unique_values) * 100[m
[32m+[m[32m            else:[m
[32m+[m[32m                casing_inconsistency_percentage = 0[m
[32m+[m[32m            metrics['Casing inconsistency percentage per attribute'][column] = casing_inconsistency_percentage[m
[32m+[m
[32m+[m[32m    return metrics[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# Assume 'df' is your pandas DataFrame[m
[32m+[m[32m# metrics = detect_and_report_casing_data_smells(df)[m
[32m+[m[32m# print(metrics)[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/duplicate_value.py b/backend/datasmells_algorithms/Vikram_smells/duplicate_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..0a006d8[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/duplicate_value.py[m
[36m@@ -0,0 +1,33 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m
[32m+[m[32mdef detect_and_report_duplicate_data(data):[m
[32m+[m[32m    # Initialize metrics dictionary to store results[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'Number of duplicate data points': 0,[m
[32m+[m[32m        'Percentage of duplicate data points': 0,[m
[32m+[m[32m        'Top 5 duplicate values per attribute': {},[m
[32m+[m[32m        'Duplicate value counts per attribute': {},[m
[32m+[m[32m        'Duplicate value distribution per attribute': {},[m
[32m+[m[32m        'Impact on analysis (qualitative)': "Not assessed"[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    # Detect duplicate data points[m
[32m+[m[32m    duplicate_rows = data[data.duplicated()][m
[32m+[m[32m    metrics['Number of duplicate data points'] = len(duplicate_rows)[m
[32m+[m[32m    metrics['Percentage of duplicate data points'] = (len(duplicate_rows) / len(data)) * 100[m
[32m+[m
[32m+[m[32m    # Loop through columns to check for duplicate values[m
[32m+[m[32m    for column in data.columns:[m
[32m+[m[32m        # Count duplicate values[m
[32m+[m[32m        duplicate_count_per_attribute = data[data.duplicated(subset=[column])][column].value_counts().to_dict()[m
[32m+[m[32m        metrics['Duplicate value counts per attribute'][column] = duplicate_count_per_attribute[m
[32m+[m
[32m+[m[32m        # Distribution of duplicate values per attribute[m
[32m+[m[32m        duplicate_distribution_per_attribute = data[data.duplicated(subset=[column])].groupby(column).size().to_dict()[m
[32m+[m[32m        metrics['Duplicate value distribution per attribute'][column] = duplicate_distribution_per_attribute[m
[32m+[m
[32m+[m[32m        # Get top 5 duplicate values for each attribute[m
[32m+[m[32m        top_duplicate_values = data[data.duplicated(subset=[column])][column].value_counts().head(5)[m
[32m+[m[32m        metrics['Top 5 duplicate values per attribute'][column] = top_duplicate_values.to_dict()[m
[32m+[m
[32m+[m[32m    return metrics[m
\ No newline at end of file[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/extreme_value.py b/backend/datasmells_algorithms/Vikram_smells/extreme_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..3af0a56[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/extreme_value.py[m
[36m@@ -0,0 +1,27 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m[32mimport numpy as np[m
[32m+[m
[32m+[m[32m# To check for extreme values in the dataset using z-score[m
[32m+[m[32mdef extreme_values(df, threshold=1.5):[m
[32m+[m[32m    instr = ''[m
[32m+[m[32m    numerical_columns = df.select_dtypes(include=['number']).columns[m
[32m+[m[32m    extreme_vals = pd.DataFrame(columns=numerical_columns)[m
[32m+[m
[32m+[m[32m    for col in numerical_columns:[m
[32m+[m[32m        z_scores = (df[col] - df[col].mean()) / df[col].std()[m
[32m+[m[32m        extreme_vals[col] = df[col][np.abs(z_scores) > threshold][m
[32m+[m
[32m+[m[32m    if extreme_vals.empty:[m
[32m+[m[32m        instr += "There are no extreme values in the dataset using z-score method with the given threshold.\n"[m
[32m+[m[32m    else:[m
[32m+[m[32m        instr += "Extreme values are present in the dataset using z-score method with the given threshold.\n"[m
[32m+[m[32m        instr += "Number of extreme values: " + str(extreme_vals.count().sum()) + "\n"[m
[32m+[m[32m        instr += "Indices of extreme values:\n"[m
[32m+[m[32m        instr += str(extreme_vals.index.tolist()) + "\n"[m
[32m+[m
[32m+[m[32m    return instr[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# Assuming 'df' is your DataFrame[m
[32m+[m[32m# instr = extreme_values(df, threshold=3)[m
[32m+[m[32m# print(instr)[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/longdata_value.py b/backend/datasmells_algorithms/Vikram_smells/longdata_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..730f1c0[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/longdata_value.py[m
[36m@@ -0,0 +1,27 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m
[32m+[m[32mdef detect_and_report_long_data_values(data, threshold=50):[m
[32m+[m[32m    # Initialize metrics dictionary to store results[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'Long data values count per attribute': {},[m
[32m+[m[32m        'Long data values percentage per attribute': {}[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    # Loop through columns to check for long data values[m
[32m+[m[32m    for column in data.columns:[m
[32m+[m[32m        if data[column].dtype == 'object':  # Check if the column contains string data[m
[32m+[m[32m            long_values_count = sum(1 for value in data[column] if isinstance(value, str) and len(value) > threshold)[m
[32m+[m[32m            metrics['Long data values count per attribute'][column] = long_values_count[m
[32m+[m[32m            total_values = len(data[column])[m
[32m+[m[32m            if total_values > 0:[m
[32m+[m[32m                long_values_percentage = (long_values_count / total_values) * 100[m
[32m+[m[32m            else:[m
[32m+[m[32m                long_values_percentage = 0[m
[32m+[m[32m            metrics['Long data values percentage per attribute'][column] = long_values_percentage[m
[32m+[m
[32m+[m[32m    return metrics[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# Assume 'df' is your pandas DataFrame[m
[32m+[m[32m# metrics = detect_and_report_long_data_values(df, threshold=50)[m
[32m+[m[32m# print(metrics)[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/mis_spelling.py b/backend/datasmells_algorithms/Vikram_smells/mis_spelling.py[m
[1mnew file mode 100644[m
[1mindex 0000000..420b568[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/mis_spelling.py[m
[36m@@ -0,0 +1,33 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m[32mimport re[m
[32m+[m
[32m+[m[32mdef detect_misspelling_data_smell(data):[m
[32m+[m[32m    # Initialize metrics dictionary to store results[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'Number of misspelled values': 0,[m
[32m+[m[32m        'Misspelled examples per attribute': {}[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    # Loop through columns to check for misspelled values[m
[32m+[m[32m    for column in data.columns:[m
[32m+[m[32m        if data[column].dtype == 'object':  # Check if the column contains strings[m
[32m+[m[32m            misspelled_values = [][m
[32m+[m[32m            misspelled_count = 0[m
[32m+[m
[32m+[m[32m            # Iterate over each value in the column[m
[32m+[m[32m            for value in data[column]:[m
[32m+[m[32m                # Perform misspelling detection logic[m
[32m+[m[32m                # Updated regex pattern to allow spaces and periods[m
[32m+[m[32m                if re.search(r'[^A-Za-z0-9\s.]+', str(value)):[m
[32m+[m[32m                    misspelled_values.append(value)[m
[32m+[m[32m                    misspelled_count += 1[m
[32m+[m
[32m+[m[32m            metrics['Number of misspelled values'] += misspelled_count[m
[32m+[m[32m            metrics['Misspelled examples per attribute'][column] = misspelled_values[m
[32m+[m
[32m+[m[32m    return metrics[m
[32m+[m
[32m+[m[32m# Example usage:[m
[32m+[m[32m# Assuming 'data' is your DataFrame[m
[32m+[m[32m# misspelling_metrics = detect_misspelling_data_smell(data)[m
[32m+[m[32m# print(misspelling_metrics)[m
[1mdiff --git a/backend/datasmells_algorithms/Vikram_smells/missing_value.py b/backend/datasmells_algorithms/Vikram_smells/missing_value.py[m
[1mnew file mode 100644[m
[1mindex 0000000..027f1ae[m
[1m--- /dev/null[m
[1m+++ b/backend/datasmells_algorithms/Vikram_smells/missing_value.py[m
[36m@@ -0,0 +1,33 @@[m
[32m+[m[32mimport pandas as pd[m
[32m+[m
[32m+[m[32mdef detect_and_report_missing_data(data):[m
[32m+[m[32m    # Initialize metrics dictionary to store results[m
[32m+[m[32m    metrics = {[m
[32m+[m[32m        'Number of missing data points': 0,[m
[32m+[m[32m        'Percentage of missing data points': 0,[m
[32m+[m[32m        'Columns with missing values': [],[m
[32m+[m[32m        'Missing value counts per column': {},[m
[32m+[m[32m        'Probability of missing values per column': {},[m
[32m+[m[32m        'Impact on analysis (qualitative)': "Not assessed"[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    # Detect missing data points[m
[32m+[m[32m    missing_values = data.isnull().sum()[m
[32m+[m[32m    total_missing = missing_values.sum()[m
[32m+[m[32m    metrics['Number of missing data points'] = int(total_missing)[m
[32m+[m[32m    metrics['Percentage of missing data points'] = (total_missing / data.size) * 100[m
[32m+[